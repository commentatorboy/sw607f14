\subsection{Test Analysis}\label{subsec:test_analysis}
The usability test turned out to display some interesting results. Some issues were expected as the implementation was not entirely done. This was clearly displayed when none of the test subjects could identify the type of pictograms from each other (Nested sequence, pictograms and choices).

On the less obvious side some of the behavior was very unexpected. A few examples of this is was a test subject attempting to drag pictograms down to make them into a choice, as well as the other test subject expecting that picking multiple pictograms would automatically make it a choice.

It was at that point we realized that the test subjects had a bias. The overall setup arranged by the requirements group involved one customer testing all applications on the same day. Test subjects expected all applications to work in similar ways whenever they seemed similar. While explaining some of the unexpected behavior, it also highlighted another issue: Consistency across applications. While the applications share components such as GIRAF\_Components and OasisLib, this does not mean that things are done the same way. Unifying similar procedures across applications could help improve the overall feel of the project and make it much more intuitive.

Some of the issues were definitely local to Sekvens, such as the issue to identify whether or not a change has been saved, and the issue of identifying the type of a pictogram. But for a solution to inconsistencies across applications, cooperation with the responsible groups would be needed.